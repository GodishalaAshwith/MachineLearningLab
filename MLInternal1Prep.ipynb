{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON6Z/Jyba+GI69+EhTw8G/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GodishalaAshwith/MachineLearningLab/blob/main/MLInternal1Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5I_Wc2JR8mf0",
        "outputId": "edf4ff79-21e3-4b30-8eb2-7d637cbe4348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Q1 Feature Selection ===\n",
            "Non-constant iris cols: Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
            "       'petal width (cm)'],\n",
            "      dtype='object')\n",
            "Quasi-constant mask: [ True  True  True  True]\n",
            "Duplicates (iris): []\n",
            "Diabetes drop cols: []\n",
            "Iris MI: {'sepal length (cm)': np.float64(0.5005752867852349), 'sepal width (cm)': np.float64(0.298200566636748), 'petal length (cm)': np.float64(0.9890186138889012), 'petal width (cm)': np.float64(0.9832532035174952)}\n",
            "Diab MI: {'age': np.float64(0.0), 'sex': np.float64(0.02191093681353684), 'bmi': np.float64(0.17472741252046653), 'bp': np.float64(0.0640651701615722), 's1': np.float64(0.0664114764761905), 's2': np.float64(0.010111224995335455), 's3': np.float64(0.07208854375985307), 's4': np.float64(0.09934996144723351), 's5': np.float64(0.15004916552529401), 's6': np.float64(0.10378776297280945)}\n",
            "Chi2 selected indices: [2 3]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SelectKBest.__init__() takes from 1 to 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-116798274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chi2 selected indices:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# 1.7 ANOVA / F-regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ANOVA iris:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_iris\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_iris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F-reg diab:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_diab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_diab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_diab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# 1.8 ROC-AUC & MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SelectKBest.__init__() takes from 1 to 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "# ================== ALL IMPORTS ==================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, label_binarize\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, f_classif, f_regression, mutual_info_classif, mutual_info_regression, RFE, SequentialFeatureSelector\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# ================== DATASETS ==================\n",
        "iris = datasets.load_iris(as_frame=True); X_iris, y_iris = iris.data, iris.target\n",
        "diab = datasets.load_diabetes(as_frame=True); X_diab, y_diab = diab.data, diab.target\n",
        "canc = datasets.load_breast_cancer(as_frame=True); X_canc, y_canc = canc.data, canc.target\n",
        "\n",
        "# ================== Q1 FEATURE SELECTION ==================\n",
        "print(\"\\n=== Q1 Feature Selection ===\")\n",
        "# 1.1 Constant\n",
        "vt = VarianceThreshold(0.0); vt.fit(X_iris)\n",
        "print(\"Non-constant iris cols:\", X_iris.columns[vt.get_support()])\n",
        "# 1.2 Quasi-constant\n",
        "vt2 = VarianceThreshold(0.01); print(\"Quasi-constant mask:\", vt2.fit(X_iris).get_support())\n",
        "# 1.3 Duplicate\n",
        "def duplicates(df):\n",
        "    dup=[]; cols=df.columns\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i+1,len(cols)):\n",
        "            if df[cols[i]].equals(df[cols[j]]): dup.append((cols[i], cols[j]))\n",
        "    return dup\n",
        "print(\"Duplicates (iris):\", duplicates(X_iris))\n",
        "# 1.4 Correlation\n",
        "corr = X_diab.corr().abs(); upper = corr.where(np.triu(np.ones(corr.shape),1).astype(bool))\n",
        "to_drop=[c for c in upper.columns if any(upper[c]>0.95)]\n",
        "print(\"Diabetes drop cols:\", to_drop)\n",
        "# 1.5 MI\n",
        "print(\"Iris MI:\", dict(zip(X_iris.columns, mutual_info_classif(X_iris, y_iris))))\n",
        "print(\"Diab MI:\", dict(zip(X_diab.columns, mutual_info_regression(X_diab, y_diab))))\n",
        "# 1.6 Chi2\n",
        "Xmm = MinMaxScaler().fit_transform(X_iris)\n",
        "chi = SelectKBest(chi2,k=2).fit(Xmm,y_iris)\n",
        "print(\"Chi2 selected indices:\", chi.get_support(indices=True))\n",
        "# 1.7 ANOVA / F-regression\n",
        "print(\"ANOVA iris:\", X_iris.columns[SelectKBest(f_classif,2).fit(X_iris,y_iris).get_support()])\n",
        "print(\"F-reg diab:\", X_diab.columns[SelectKBest(f_regression,3).fit(X_diab,y_diab).get_support()])\n",
        "# 1.8 ROC-AUC & MSE\n",
        "y_bin = label_binarize(y_iris, classes=np.unique(y_iris))\n",
        "aucs={}\n",
        "for col in X_iris.columns:\n",
        "    try:\n",
        "        aucs[col]=np.mean([roc_auc_score(y_bin[:,c],X_iris[col]) for c in range(y_bin.shape[1])])\n",
        "    except: aucs[col]=np.nan\n",
        "print(\"Iris AUC:\", aucs)\n",
        "mses={}\n",
        "for col in X_diab.columns:\n",
        "    lr=LinearRegression().fit(X_diab[[col]],y_diab)\n",
        "    mses[col]=mean_squared_error(y_diab, lr.predict(X_diab[[col]]))\n",
        "print(\"Diab MSE:\", mses)\n",
        "\n",
        "# ================== Q2 WRAPPER METHODS ==================\n",
        "print(\"\\n=== Q2 Wrapper Methods ===\")\n",
        "Xtr,Xte,ytr,yte=train_test_split(X_iris,y_iris,test_size=0.3,random_state=0)\n",
        "est=LogisticRegression(max_iter=500)\n",
        "# Forward\n",
        "sfs_fwd=SequentialFeatureSelector(est,n_features_to_select=2,direction='forward').fit(Xtr,ytr)\n",
        "print(\"Forward:\", Xtr.columns[sfs_fwd.get_support()])\n",
        "# Backward\n",
        "sfs_bwd=SequentialFeatureSelector(est,n_features_to_select=2,direction='backward').fit(Xtr,ytr)\n",
        "print(\"Backward:\", Xtr.columns[sfs_bwd.get_support()])\n",
        "# Exhaustive\n",
        "def exhaustive(X,y,model,kmax=3):\n",
        "    best,bs=-1,None\n",
        "    for k in range(1,min(kmax,len(X.columns))+1):\n",
        "        for combo in itertools.combinations(X.columns,k):\n",
        "            score=np.mean(cross_val_score(model,X[list(combo)],y,cv=3))\n",
        "            if score>best: best,bs=score,combo\n",
        "    return bs,best\n",
        "print(\"Exhaustive best:\", exhaustive(X_iris,y_iris,LogisticRegression(max_iter=500),3))\n",
        "# RFE\n",
        "rfe=RFE(estimator=LogisticRegression(max_iter=500),n_features_to_select=2).fit(Xtr,ytr)\n",
        "print(\"RFE ranking:\", dict(zip(Xtr.columns, rfe.ranking_)))\n",
        "\n",
        "# ================== Q3 EMBEDDED METHODS ==================\n",
        "print(\"\\n=== Q3 Embedded Methods ===\")\n",
        "lasso=Lasso(0.1).fit(X_diab,y_diab); ridge=Ridge(1.0).fit(X_diab,y_diab); en=ElasticNet(0.1,l1_ratio=0.5).fit(X_diab,y_diab)\n",
        "print(\"Lasso coefs:\", lasso.coef_); print(\"Ridge coefs:\", ridge.coef_); print(\"EN coefs:\", en.coef_)\n",
        "rf=RandomForestClassifier(n_estimators=50).fit(X_canc,y_canc)\n",
        "print(\"RF top5:\", sorted(zip(X_canc.columns,rf.feature_importances_),key=lambda x:-x[1])[:5])\n",
        "dt=DecisionTreeClassifier().fit(X_canc,y_canc)\n",
        "print(\"DT top5:\", sorted(zip(X_canc.columns,dt.feature_importances_),key=lambda x:-x[1])[:5])\n",
        "sv=SVC(kernel='linear').fit(X_canc,y_canc)\n",
        "print(\"SVC coef abs top5:\", sorted(zip(X_canc.columns,np.abs(sv.coef_.ravel())),key=lambda x:-x[1])[:5])\n",
        "\n",
        "# ================== Q4 PCA ==================\n",
        "print(\"\\n=== Q4 PCA ===\")\n",
        "Xs=StandardScaler().fit_transform(X_diab); pca=PCA(n_components=10); Xp=pca.fit_transform(Xs)\n",
        "plt.scatter(Xp[:,0],Xp[:,1],s=8); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"PCA PC1 vs PC2\"); plt.show()\n",
        "loadings=pd.DataFrame(pca.components_,columns=X_diab.columns)\n",
        "print(\"PC1 top:\", loadings.iloc[0].abs().sort_values(ascending=False).head())\n",
        "print(\"PC2 top:\", loadings.iloc[1].abs().sort_values(ascending=False).head())\n",
        "print(\"Explained ratios:\", pca.explained_variance_ratio_[:5])\n",
        "print(\"PC1+PC2 %:\", (pca.explained_variance_ratio_[0]+pca.explained_variance_ratio_[1])*100)\n",
        "cum=np.cumsum(PCA().fit(Xs).explained_variance_ratio_)\n",
        "print(\"PCs >=80%:\", np.searchsorted(cum,0.8)+1)\n",
        "\n",
        "# ================== Q5 LINEAR REGRESSION ==================\n",
        "print(\"\\n=== Q5 Linear Regression (House Price) ===\")\n",
        "cal=fetch_california_housing(as_frame=True); X_cal,y_cal=cal.data[['MedInc']],cal.target\n",
        "lr=LinearRegression().fit(X_cal,y_cal); y_pred=lr.predict(X_cal)\n",
        "print(\"Intercept:\", lr.intercept_,\"Slope:\", lr.coef_[0])\n",
        "print(\"MSE:\", mean_squared_error(y_cal,y_pred),\"MAE:\", mean_absolute_error(y_cal,y_pred),\"RMSE:\", mean_squared_error(y_cal,y_pred,squared=False))\n",
        "\n",
        "# ================== Q6 SALARY DATA ==================\n",
        "print(\"\\n=== Q6 Salary_Data.csv ===\")\n",
        "try:\n",
        "    sal=pd.read_csv(\"Salary_Data.csv\"); Xs,ys=sal[['YearsExperience']],sal['Salary']\n",
        "    lr_sal=LinearRegression().fit(Xs,ys)\n",
        "    print(\"Intercept:\", lr_sal.intercept_,\"Slope:\", lr_sal.coef_[0])\n",
        "except Exception as e: print(\"Salary_Data.csv missing:\", e)\n",
        "\n",
        "# ================== Q7 MULTIPLE LINEAR REGRESSION ==================\n",
        "print(\"\\n=== Q7 Multiple Linear Regression ===\")\n",
        "np.random.seed(0); n=200\n",
        "Age=np.random.randint(18,60,n); Height=np.random.normal(165,10,n); Weight=np.random.normal(70,12,n); TimeOnExercise=np.random.normal(30,8,n)\n",
        "WeightLoss=0.12*TimeOnExercise-0.01*Age+np.random.normal(0,1,n)\n",
        "df=pd.DataFrame({'Age':Age,'Height':Height,'Weight':Weight,'TimeOnExercise':TimeOnExercise,'WeightLoss':WeightLoss})\n",
        "Xw,yw=df[['Age','Height','Weight','TimeOnExercise']],df['WeightLoss']\n",
        "mlr=LinearRegression().fit(Xw,yw)\n",
        "print(\"WeightLoss coeffs:\", dict(zip(Xw.columns,mlr.coef_)))\n",
        "Xh=cal.data[['MedInc','HouseAge','AveRooms']]; yh=cal.target\n",
        "mlr2=LinearRegression().fit(Xh,yh)\n",
        "print(\"House coeffs:\", dict(zip(Xh.columns,mlr2.coef_)))\n",
        "\n",
        "# ================== Q8 POLYNOMIAL REGRESSION ==================\n",
        "print(\"\\n=== Q8 Polynomial Regression ===\")\n",
        "X=np.linspace(-3,3,200).reshape(-1,1); y=X.ravel()**3+np.random.normal(0,3,200)\n",
        "poly=PolynomialFeatures(3); Xp=poly.fit_transform(X); pr=LinearRegression().fit(Xp,y)\n",
        "print(\"Poly coefs:\", pr.coef_)\n",
        "plt.scatter(X,y,s=6); xx=np.linspace(-3,3,200).reshape(-1,1); plt.plot(xx,pr.predict(poly.transform(xx))); plt.show()\n",
        "\n",
        "# ================== Q9 LOGISTIC REGRESSION ==================\n",
        "print(\"\\n=== Q9 Logistic Regression ===\")\n",
        "Xtr,Xte,ytr,yte=train_test_split(X_canc,y_canc,test_size=0.3,random_state=0)\n",
        "log=LogisticRegression(max_iter=1000).fit(Xtr,ytr); yp=log.predict(Xte)\n",
        "cm=confusion_matrix(yte,yp); print(\"CM:\\n\",cm)\n",
        "if cm.size==4:\n",
        "    tn,fp,fn,tp=cm.ravel(); acc=(tp+tn)/(tp+tn+fp+fn)\n",
        "    prec=tp/(tp+fp) if tp+fp>0 else 0; rec=tp/(tp+fn) if tp+fn>0 else 0\n",
        "    f1=2*prec*rec/(prec+rec) if prec+rec>0 else 0\n",
        "    print(\"Acc:\",acc,\"Prec:\",prec,\"Rec:\",rec,\"F1:\",f1)\n",
        "print(classification_report(yte,yp))\n",
        "\n",
        "# ================== Q10 GRADIENT DESCENT ==================\n",
        "print(\"\\n=== Q10 Gradient Descent ===\")\n",
        "Xg=X_cal.values.flatten(); y=y_cal; Xg=(Xg-Xg.mean())/Xg.std(); Xb=np.vstack([np.ones_like(Xg),Xg]).T\n",
        "def grad_desc(X,y,lr=0.01,epochs=1000):\n",
        "    theta=np.zeros(X.shape[1]); m=len(y); hist=[]\n",
        "    for i in range(epochs):\n",
        "        grad=(1/m)*X.T.dot(X.dot(theta)-y); theta=theta-lr*grad\n",
        "        if i%(epochs//5 or 1)==0: hist.append((i,mean_squared_error(y,X.dot(theta))))\n",
        "    return theta,hist\n",
        "for lr in [0.001,0.01,0.1]:\n",
        "    theta,hist=grad_desc(Xb,y,lr,2000)\n",
        "    print(\"lr\",lr,\"theta:\",theta,\"sample history:\",hist[:3])\n",
        "\n",
        "# ================== Q11 REGULARIZATION ==================\n",
        "print(\"\\n=== Q11 Regularization ===\")\n",
        "Xn=np.linspace(-3,3,200).reshape(-1,1); yn=Xn.ravel()**3+np.random.normal(0,4,200)\n",
        "poly10=PolynomialFeatures(10); Xn10=poly10.fit_transform(Xn)\n",
        "lr_over=LinearRegression().fit(Xn10,yn); mse_over=mean_squared_error(yn,lr_over.predict(Xn10))\n",
        "lass=Lasso(0.1,max_iter=5000).fit(Xn10,yn); rid=Ridge(1.0).fit(Xn10,yn)\n",
        "print(\"MSE overfit:\",mse_over,\"MSE Lasso:\",mean_squared_error(yn,lass.predict(Xn10)),\"MSE Ridge:\",mean_squared_error(yn,rid.predict(Xn10)))\n",
        "print(\"Lasso non-zero:\",np.sum(lass.coef_!=0))\n",
        "\n",
        "# ================== Q12 DECISION TREE ==================\n",
        "print(\"\\n=== Q12 Decision Tree ===\")\n",
        "Xtr,Xte,ytr,yte=train_test_split(X_diab,y_diab,test_size=0.3,random_state=0)\n",
        "dtr=DecisionTreeRegressor(max_depth=4).fit(Xtr,ytr)\n",
        "print(export_text(dtr,feature_names=list(X_diab.columns)))\n",
        "plt.figure(figsize=(12,6)); plot_tree(dtr,feature_names=X_diab.columns,filled=True); plt.show()\n",
        "\n",
        "# ================== Q13 NAIVE BAYES ==================\n",
        "print(\"\\n=== Q13 Naive Bayes ===\")\n",
        "try:\n",
        "    loan=pd.read_csv(\"loan_data.csv\").dropna()\n",
        "    Xloan=loan.select_dtypes(include=[np.number])\n",
        "    yloan=(loan['Loan_Status']=='Y').astype(int)\n",
        "    Xtr,Xte,ytr,yte=train_test_split(Xloan,yloan,test_size=0.3,random_state=0)\n",
        "    nb=GaussianNB().fit(Xtr,ytr); print(\"NB acc:\",nb.score(Xte,yte))\n",
        "except Exception as e: print(\"loan_data.csv missing:\", e)\n",
        "\n",
        "# ================== Q14 SVC ==================\n",
        "print(\"\\n=== Q14 SVC ===\")\n",
        "Xtr,Xte,ytr,yte=train_test_split(X_canc,y_canc,test_size=0.3,random_state=0)\n",
        "svc=SVC(kernel='rbf').fit(Xtr,ytr); print(\"SVC acc:\",svc.score(Xte,yte))\n",
        "\n",
        "# ================== Q15 KNN ==================\n",
        "print(\"\\n=== Q15 KNN ===\")\n",
        "Xtr,Xte,ytr,yte=train_test_split(X_iris,y_iris,test_size=0.3,random_state=0)\n",
        "knn=KNeighborsClassifier(3).fit(Xtr,ytr); print(\"KNN acc:\",knn.score(Xte,yte))\n",
        "\n",
        "print(\"\\n=== DONE ALL QUESTIONS ===\")\n"
      ]
    }
  ]
}