{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13299173,"sourceType":"datasetVersion","datasetId":8429390}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"ensemble learning","metadata":{}},{"cell_type":"code","source":"#preprocessing\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv('/kaggle/input/diabetes-ensemble/diabetes.csv')\n\ncols_with_zero_as_missing = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\ndf[cols_with_zero_as_missing] = df[cols_with_zero_as_missing].replace(0, np.nan)\n\nfor col in cols_with_zero_as_missing:\n    df[col] = df[col].fillna(df[col].median())  \n\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Preprocessing complete.\")\nprint(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\nprint(f\"Positive cases in train: {sum(y_train)}, in test: {sum(y_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:09:08.033669Z","iopub.execute_input":"2025-10-12T15:09:08.034061Z","iopub.status.idle":"2025-10-12T15:09:08.062681Z","shell.execute_reply.started":"2025-10-12T15:09:08.034021Z","shell.execute_reply":"2025-10-12T15:09:08.061163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndt = DecisionTreeClassifier(random_state=0) #decision tree ki usually we give 0, for train_test_split we give 42\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n\n\ndt_metrics = {\n    'Accuracy': accuracy_score(y_test, y_pred_dt),\n    'Precision': precision_score(y_test, y_pred_dt),\n    'Recall': recall_score(y_test, y_pred_dt),\n    'F1-Score': f1_score(y_test, y_pred_dt)\n}\n\n#decision tree \nprint(\"Decision Tree Performance:\")\nfor metric, value in dt_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\n#random forest classifier\nn_estimators_range = [1, 5, 10, 20, 50, 100]\nrf_metrics = {'n_estimators': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': []}\n\nfor n in n_estimators_range:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n    rf.fit(X_train, y_train)\n    y_pred_rf = rf.predict(X_test)\n    \n    rf_metrics['n_estimators'].append(n)\n    rf_metrics['Accuracy'].append(accuracy_score(y_test, y_pred_rf))\n    rf_metrics['Precision'].append(precision_score(y_test, y_pred_rf))\n    rf_metrics['Recall'].append(recall_score(y_test, y_pred_rf))\n    rf_metrics['F1-Score'].append(f1_score(y_test, y_pred_rf))\n\nprint(\"\\nRandom Forest Performance with different number of estimators:\")\nfor i, n in enumerate(rf_metrics['n_estimators']):\n    print(f\"n_estimators={n} | Accuracy={rf_metrics['Accuracy'][i]:.4f}, Precision={rf_metrics['Precision'][i]:.4f}, Recall={rf_metrics['Recall'][i]:.4f}, F1-Score={rf_metrics['F1-Score'][i]:.4f}\")\n\n# Plotting the effect of number of estimators on metrics\nplt.figure(figsize=(10,6))\nplt.plot(rf_metrics['n_estimators'], rf_metrics['Accuracy'], label='Accuracy')\nplt.plot(rf_metrics['n_estimators'], rf_metrics['Precision'], label='Precision')\nplt.plot(rf_metrics['n_estimators'], rf_metrics['Recall'], label='Recall')\nplt.plot(rf_metrics['n_estimators'], rf_metrics['F1-Score'], label='F1-Score')\nplt.xlabel('Number of Estimators')\nplt.ylabel('Score')\nplt.title('Random Forest Performance vs Number of Estimators')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:09:08.064237Z","iopub.execute_input":"2025-10-12T15:09:08.064508Z","iopub.status.idle":"2025-10-12T15:09:08.850918Z","shell.execute_reply.started":"2025-10-12T15:09:08.064488Z","shell.execute_reply":"2025-10-12T15:09:08.849762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. What differences do you observe between the Decision Tree and Random Forest results?\n\nThe Random Forest consistently outperforms the single Decision Tree across all metrics (accuracy, precision, recall, F1-score). The Decision Tree tends to overfit the training data and may have lower generalization ability, leading to lower test performance.\nRandom Forest, by aggregating multiple trees, reduces variance and improves robustness, yielding better overall predictive power.","metadata":{}},{"cell_type":"markdown","source":"2. How does increasing the number of estimators affect performance and stability?\n\nIncreasing the number of estimators (trees) in Random Forest generally improves model stability and performance, as the ensemble averages out errors/noise from individual trees. Performance gains tend to plateau after a certain number of trees (e.g., 50-100 trees), where adding more trees provides diminishing returns but stabilizes predictions. More trees reduce variance but come at a computational cost.","metadata":{}},{"cell_type":"markdown","source":"3. Why does Random Forest generally perform better than a single Decision Tree?\n\nRandom Forest introduces bagging (bootstrap aggregating), training each tree on a random sample of data, reducing variance. It also randomly selects a subset of features for splitting at each node, adding feature randomness and decorrelating trees. This diversity among trees means the ensemble can average out errors and reduce overfitting, leading to better generalization compared to a single Decision Tree.","metadata":{}},{"cell_type":"code","source":"#2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\n# Initialize base classifiers\ndt = DecisionTreeClassifier(random_state=0)\nlr = LogisticRegression(random_state=42, max_iter=500)\nknn = KNeighborsClassifier()\n\n# Train base classifiers\ndt.fit(X_train, y_train)\nlr.fit(X_train, y_train)\nknn.fit(X_train, y_train)\n\n# Get predictions and predicted probabilities\npred_dt = dt.predict(X_test)\npred_lr = lr.predict(X_test)\npred_knn = knn.predict(X_test)\n\nprob_dt = dt.predict_proba(X_test)[:, 1]\nprob_lr = lr.predict_proba(X_test)[:, 1]\nprob_knn = knn.predict_proba(X_test)[:, 1]\n\n# 1. Max Voting (hard voting by majority class)\n# Combine predictions into array\npreds = np.vstack([pred_dt, pred_lr, pred_knn])\nmax_voting_pred = (np.sum(preds, axis=0) >= 2).astype(int)  # Majority vote\n\n# 2. Average Voting (soft voting by averaging probabilities)\navg_prob = (prob_dt + prob_lr + prob_knn) / 3\navg_voting_pred = (avg_prob >= 0.5).astype(int)\n\n# 3. Weighted Average Voting\n\n# Calculate weights based on individual classifier accuracy on test set\n\nacc_dt = accuracy_score(y_test, pred_lr)\nacc_lr = accuracy_score(y_test, pred_lr)\nacc_knn = accuracy_score(y_test, pred_knn)\n\nweights = np.array([acc_dt, acc_lr, acc_knn])\nweights = weights / weights.sum()  # Normalize weights\n\nweighted_prob = weights[0]*prob_dt + weights[1]*prob_lr + weights[2]*prob_knn\nweighted_voting_pred = (weighted_prob >= 0.5).astype(int)\n\n# Evaluation function\ndef evaluate(y_true, y_pred, label):\n    print(f\"{label}:\")\n    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n    print()\n\n# Evaluate base classifiers\nevaluate(y_test, pred_dt, \"Decision Tree\")\nevaluate(y_test, pred_lr, \"Logistic Regression\")\nevaluate(y_test, pred_knn, \"K-Nearest Neighbors\")\n\n# Evaluate ensembles\nevaluate(y_test, max_voting_pred, \"Max Voting Ensemble\")\nevaluate(y_test, avg_voting_pred, \"Average Voting Ensemble\")\nevaluate(y_test, weighted_voting_pred, \"Weighted Average Voting Ensemble\")\n\nprint(f\"Weights used in weighted voting: Decision Tree={weights[0]:.3f}, Logistic Regression={weights[1]:.3f}, KNN={weights[2]:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:09:08.851926Z","iopub.execute_input":"2025-10-12T15:09:08.852182Z","iopub.status.idle":"2025-10-12T15:09:08.992152Z","shell.execute_reply.started":"2025-10-12T15:09:08.852162Z","shell.execute_reply":"2025-10-12T15:09:08.991385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. How do the results vary between max voting, average voting, and weighted voting?\n\nMax Voting treats each base classifier’s prediction equally and picks the majority class; it can be less sensitive to confidence scores. Average Voting uses probabilities, often resulting in smoother, better calibrated decisions since it aggregates confidence.\nWeighted Voting further improves average voting by giving more influence to stronger classifiers, often yielding the best performance among the three.","metadata":{}},{"cell_type":"markdown","source":"2. What is the role of weights in improving ensemble predictions?\n\nWeights reflect each base model’s reliability; better-performing models contribute more to the final prediction. This allows the ensemble to leverage the strengths of more accurate classifiers and reduce influence of weaker ones, improving overall accuracy and robustness.","metadata":{}},{"cell_type":"markdown","source":"3. Which base classifiers combine most effectively under each voting scheme?\n\nClassifiers with complementary strengths (e.g., Decision Tree capturing non-linear patterns, Logistic Regression providing stable linear decision boundaries, and KNN leveraging local instance information) typically combine well. Under max voting, diversity is key as all votes are equal. Under average and weighted voting, classifiers that produce well-calibrated probabilities (like Logistic Regression) generally contribute more effectively.","metadata":{}},{"cell_type":"code","source":"#3\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Initialize base classifiers\ndt = DecisionTreeClassifier(random_state=42)\nlr = LogisticRegression(random_state=42, max_iter=500)\nknn = KNeighborsClassifier()\n\n# Hard Voting Ensemble\nhard_voting_clf = VotingClassifier(\n    estimators=[('dt', dt), ('lr', lr), ('knn', knn)],\n    voting='hard'\n)\nhard_voting_clf.fit(X_train, y_train)\ny_pred_hard = hard_voting_clf.predict(X_test)\n\n# Soft Voting Ensemble\nsoft_voting_clf = VotingClassifier(\n    estimators=[('dt', dt), ('lr', lr), ('knn', knn)],\n    voting='soft'\n)\nsoft_voting_clf.fit(X_train, y_train)\ny_pred_soft = soft_voting_clf.predict(X_test)\n\n# Evaluation function\ndef evaluate(y_true, y_pred, label):\n    print(f\"{label}:\")\n    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n    print()\n\n# Evaluate and compare\nevaluate(y_test, y_pred_hard, \"Hard Voting Classifier\")\nevaluate(y_test, y_pred_soft, \"Soft Voting Classifier\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:09:08.994003Z","iopub.execute_input":"2025-10-12T15:09:08.994766Z","iopub.status.idle":"2025-10-12T15:09:09.155104Z","shell.execute_reply.started":"2025-10-12T15:09:08.994736Z","shell.execute_reply":"2025-10-12T15:09:09.154251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. What is the main difference in prediction mechanisms between hard and soft voting?\n\nHard voting predicts the class that gets the majority vote (mode) from base classifiers’ predicted classes. Soft voting predicts the class with the highest average predicted probability across all base classifiers. Soft voting uses probability estimates, while hard voting uses discrete class labels.","metadata":{}},{"cell_type":"markdown","source":"2. In which situations does soft voting outperform hard voting?\n\nSoft voting usually outperforms hard voting when base classifiers are well-calibrated and output reliable probability estimates. It leverages confidence levels to make more nuanced decisions, especially when classifiers disagree. Soft voting is beneficial when classifiers have varying certainty in their predictions.","metadata":{}},{"cell_type":"markdown","source":"3. How does the probability calibration of base classifiers influence soft voting?\n\nIf base classifiers produce poorly calibrated probabilities (e.g., decision trees often do), soft voting may be less effective or biased. Better-calibrated models (like Logistic Regression) improve soft voting by providing meaningful probability scores. Sometimes, calibration techniques (e.g., Platt scaling or isotonic regression) are used to improve soft voting effectiveness.","metadata":{}},{"cell_type":"code","source":"#4\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\n\n# Parameter grids\nn_estimators_list = [1, 10, 50, 100]\nmax_depth_list = [None, 5, 10, 20]\nrandom_states = [0, 42, 100]\n\nresults = []\n\nfor rs in random_states:\n    for max_d in max_depth_list:\n        for n_est in n_estimators_list:\n            rf = RandomForestClassifier(\n                n_estimators=n_est,\n                max_depth=max_d,\n                random_state=rs\n            )\n            rf.fit(X_train, y_train)\n            y_pred = rf.predict(X_test)\n            results.append({\n                'random_state': rs,\n                'max_depth': max_d,\n                'n_estimators': n_est,\n                'accuracy': accuracy_score(y_test, y_pred),\n                'precision': precision_score(y_test, y_pred),\n                'recall': recall_score(y_test, y_pred),\n                'f1_score': f1_score(y_test, y_pred)\n            })\n\n# Convert to DataFrame for easier analysis\nimport pandas as pd\ndf_results = pd.DataFrame(results)\n\n# Show summary\nprint(df_results)\n\n# Plot effect of n_estimators for a fixed random_state and max_depth=None\nsubset = df_results[(df_results['random_state'] == 42) & (df_results['max_depth'].isnull() | (df_results['max_depth'].isna()))]\n\nplt.figure(figsize=(10,6))\nplt.plot(subset['n_estimators'], subset['accuracy'], marker='o')\nplt.title('Effect of n_estimators on Accuracy (max_depth=None, random_state=42)')\nplt.xlabel('Number of Estimators')\nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.show()\n\n# Plot effect of max_depth for fixed n_estimators=100 and random_state=42\nsubset_depth = df_results[(df_results['random_state'] == 42) & (df_results['n_estimators'] == 100)]\n\nplt.figure(figsize=(10,6))\nplt.plot(subset_depth['max_depth'], subset_depth['accuracy'], marker='o')\nplt.title('Effect of max_depth on Accuracy (n_estimators=100, random_state=42)')\nplt.xlabel('Max Depth')\nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:09:09.155791Z","iopub.execute_input":"2025-10-12T15:09:09.15607Z","iopub.status.idle":"2025-10-12T15:09:14.532261Z","shell.execute_reply.started":"2025-10-12T15:09:09.156046Z","shell.execute_reply":"2025-10-12T15:09:14.530104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. How do different values of n_estimators and max_depth affect model performance?\n\nIncreasing n_estimators generally improves performance and stabilizes the model by reducing variance because more trees mean more averaging. After a certain point (e.g., 50-100 trees), gains plateau and computational cost increases.\n\nmax_depth controls tree complexity:\nSmaller max_depth can reduce overfitting by limiting complexity but may cause underfitting if too low.\nLarger max_depth allows trees to fit training data better but risks overfitting.\nOptimal max_depth balances bias and variance, usually found through experimentation.","metadata":{}},{"cell_type":"markdown","source":"2. What does Bagging achieve in terms of variance and bias reduction?\n\nBagging (Bootstrap Aggregating) primarily reduces variance by training each base learner on random subsets of data and aggregating their predictions. It does not significantly reduce bias; if base learners are biased, bagging won't fix that. The ensemble’s variance is lower than that of individual models, resulting in improved generalization and stability. This means that we can reduce","metadata":{}},{"cell_type":"markdown","source":"3. How does Random Forest handle overfitting compared to a single Decision Tree?\n\nRandom Forest mitigates overfitting by combining many trees trained on bootstrapped samples and considering random feature subsets for splits, increasing model diversity. This averaging effect reduces the variance and makes the ensemble less prone to overfitting compared to a single deep Decision Tree, which can perfectly fit training data but generalize poorly. Overall, Random Forest strikes a balance between low bias and reduced variance, improving test performance.","metadata":{}},{"cell_type":"code","source":"#5\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load regression dataset\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Experiment with different numbers of trees\nn_estimators_list = [10, 50, 100, 200]\n\noob_scores = []\ntest_r2_scores = []\n\nfor n_est in n_estimators_list:\n    rf_reg = RandomForestRegressor(\n        n_estimators=n_est,\n        oob_score=True,\n        random_state=42,\n        bootstrap=True  # Required for OOB\n    )\n    rf_reg.fit(X_train, y_train)\n    \n    # Predictions on test set\n    y_pred = rf_reg.predict(X_test)\n    \n    # Record OOB score and test R2 score\n    oob_scores.append(rf_reg.oob_score_)\n    test_r2_scores.append(r2_score(y_test, y_pred))\n\n# Display results neatly\nprint(\"=== Random Forest Regressor Performance ===\")\nfor n, oob, r2 in zip(n_estimators_list, oob_scores, test_r2_scores):\n    print(f\"n_estimators = {n:3d} | OOB Score = {oob:.4f} | Test R² Score = {r2:.4f}\")\n\n# Optional: Visual comparison plot\nplt.figure(figsize=(8,5))\nplt.plot(n_estimators_list, oob_scores, marker='o', label='OOB Score')\nplt.plot(n_estimators_list, test_r2_scores, marker='s', label='Test R² Score')\nplt.xlabel(\"Number of Trees (n_estimators)\")\nplt.ylabel(\"Score\")\nplt.title(\"Effect of Number of Trees on Random Forest Performance\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:10:28.017241Z","iopub.execute_input":"2025-10-12T15:10:28.017565Z","iopub.status.idle":"2025-10-12T15:11:29.336808Z","shell.execute_reply.started":"2025-10-12T15:10:28.017544Z","shell.execute_reply":"2025-10-12T15:11:29.335677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. What does the oob_score_ indicate about model performance?\n\nThe oob_score_ is an estimate of the model’s generalization performance calculated using samples not included in the bootstrap sample for each tree (out-of-bag samples). It approximates test-set performance without needing a separate validation set, providing an unbiased measure of prediction accuracy (R² for regression).","metadata":{}},{"cell_type":"markdown","source":"2. How does Out-of-Bag evaluation differ from traditional test-set evaluation?\n\nTraditional evaluation requires splitting data into training and test sets, training the model on one and testing on the other. OOB evaluation uses the inherent bootstrap sampling in Random Forest: each tree trains on a subset, and predictions on unused samples are aggregated for performance metrics. OOB evaluation is more efficient, uses all data for training, and reduces the need for an explicit validation split.","metadata":{}},{"cell_type":"markdown","source":"3. How does the OOB score change with different numbers of trees?\n\nOOB score generally improves and stabilizes as the number of trees increases because more trees reduce variance and increase prediction stability. Initially, with few trees, OOB scores may fluctuate or be less accurate. After a certain number (e.g., 100+ trees), OOB scores plateau, indicating diminishing returns from adding more trees.","metadata":{}},{"cell_type":"code","source":"#6\n# ===========================================================\n# Exploring Boosting Techniques on Preprocessed Diabetes Data\n# ===========================================================\n\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# -----------------------------------------------------------\n# Ensure y is binary (classification-compatible)\n# -----------------------------------------------------------\n# Some preprocessing pipelines scale y by mistake; revert to 0/1\ny_train = np.rint(y_train).astype(int)\ny_test = np.rint(y_test).astype(int)\n\n# ---------------------------\n# 1. Adaptive Boosting (AdaBoost)\n# ---------------------------\nprint(\"=== AdaBoost Classifier ===\")\nadaboost = AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42)\nadaboost.fit(X_train, y_train)\ny_pred_ada = adaboost.predict(X_test)\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred_ada), 4))\nprint(\"Observation: AdaBoost builds models sequentially, giving more importance to misclassified samples \"\n      \"so that the next weak learner can correct previous mistakes.\\n\")\n\n# ---------------------------\n# 2. Gradient Boosting\n# ---------------------------\nprint(\"=== Gradient Boosting Classifier ===\")\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\ngb.fit(X_train, y_train)\ny_pred_gb = gb.predict(X_test)\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred_gb), 4))\nprint(\"Observation: Gradient Boosting fits each new model on the residuals (errors) of the previous model. \"\n      \"This allows it to minimize a chosen loss function and be more flexible than AdaBoost.\\n\")\n\n# ---------------------------\n# 3. XGBoost\n# ---------------------------\nprint(\"=== XGBoost Classifier ===\")\nxgb = XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred_xgb), 4))\nprint(\"Observation: XGBoost improves Gradient Boosting by adding regularization (L1/L2), \"\n      \"parallel processing, and efficient tree growing, leading to faster training and better generalization.\\n\")\n\n# ---------------------------\n# 4. CatBoost\n# ---------------------------\nprint(\"=== CatBoost Classifier ===\")\ncat = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\ncat.fit(X_train, y_train)\ny_pred_cat = cat.predict(X_test)\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred_cat), 4))\nprint(\"Observation: CatBoost automatically handles categorical features, uses ordered boosting \"\n      \"to reduce overfitting, and usually converges faster with less parameter tuning.\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:14:50.306374Z","iopub.execute_input":"2025-10-12T15:14:50.306742Z","iopub.status.idle":"2025-10-12T15:15:35.166321Z","shell.execute_reply.started":"2025-10-12T15:14:50.306716Z","shell.execute_reply":"2025-10-12T15:15:35.165145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"AdaBoost increases the weights of misclassified samples after each iteration, \"\n      \"forcing later models to pay more attention to those difficult cases.","metadata":{}},{"cell_type":"markdown","source":"Gradient Boosting minimizes any differentiable loss function, not just weighted errors, \"\n      \"so it can handle both classification and regression tasks effectively.","metadata":{}},{"cell_type":"markdown","source":"XGBoost uses parallel computation, regularization, and efficient memory usage, \"\n      \"while CatBoost uses ordered boosting and automatic handling of categorical features for faster, accurate training.","metadata":{}},{"cell_type":"markdown","source":"In most cases, XGBoost offers the best trade-off—it trains faster than AdaBoost and Gradient Boosting, \"\n      \"and achieves accuracy comparable to or better than CatBoost.","metadata":{}},{"cell_type":"code","source":"#7\n# ===============================================================\n# Comparative Study of Ensemble Models - Diabetes Dataset\n# ===============================================================\n\nimport time\nimport pandas as pd\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    RandomForestClassifier,\n    AdaBoostClassifier,\n    GradientBoostingClassifier\n)\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Store results\nresults = []\n\n# ----------------------------------------------------------\n# 1. Bagging Classifier\n# ----------------------------------------------------------\nstart = time.time()\nbagging = BaggingClassifier(n_estimators=100, random_state=42)\nbagging.fit(X_train, y_train)\ny_pred_bag = bagging.predict(X_test)\nend = time.time()\nresults.append({\n    \"Model\": \"Bagging\",\n    \"Accuracy\": round(accuracy_score(y_test, y_pred_bag), 4),\n    \"Training Time (s)\": round(end - start, 3),\n    \"Interpretability\": \"Low - multiple base estimators\",\n    \"Overfitting Behavior\": \"Low to moderate, depends on base model\"\n})\n\n# ----------------------------------------------------------\n# 2. Random Forest\n# ----------------------------------------------------------\nstart = time.time()\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nend = time.time()\nresults.append({\n    \"Model\": \"Random Forest\",\n    \"Accuracy\": round(accuracy_score(y_test, y_pred_rf), 4),\n    \"Training Time (s)\": round(end - start, 3),\n    \"Interpretability\": \"Medium - feature importance available\",\n    \"Overfitting Behavior\": \"Low - uses averaging of multiple trees\"\n})\n\n# ----------------------------------------------------------\n# 3. AdaBoost\n# ----------------------------------------------------------\nstart = time.time()\nada = AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42)\nada.fit(X_train, y_train)\ny_pred_ada = ada.predict(X_test)\nend = time.time()\nresults.append({\n    \"Model\": \"AdaBoost\",\n    \"Accuracy\": round(accuracy_score(y_test, y_pred_ada), 4),\n    \"Training Time (s)\": round(end - start, 3),\n    \"Interpretability\": \"Low - complex weight updates\",\n    \"Overfitting Behavior\": \"Can overfit noisy data if too many estimators\"\n})\n\n# ----------------------------------------------------------\n# 4. Gradient Boosting\n# ----------------------------------------------------------\nstart = time.time()\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\ngb.fit(X_train, y_train)\ny_pred_gb = gb.predict(X_test)\nend = time.time()\nresults.append({\n    \"Model\": \"Gradient Boosting\",\n    \"Accuracy\": round(accuracy_score(y_test, y_pred_gb), 4),\n    \"Training Time (s)\": round(end - start, 3),\n    \"Interpretability\": \"Medium - feature importance possible\",\n    \"Overfitting Behavior\": \"Can overfit if not tuned carefully\"\n})\n\n# ----------------------------------------------------------\n# 5. XGBoost\n# ----------------------------------------------------------\nstart = time.time()\nxgb = XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nend = time.time()\nresults.append({\n    \"Model\": \"XGBoost\",\n    \"Accuracy\": round(accuracy_score(y_test, y_pred_xgb), 4),\n    \"Training Time (s)\": round(end - start, 3),\n    \"Interpretability\": \"Medium - has feature importance & SHAP support\",\n    \"Overfitting Behavior\": \"Low due to regularization\"\n})\n\n# ----------------------------------------------------------\n# 6. CatBoost\n# ----------------------------------------------------------\nstart = time.time()\ncat = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\ncat.fit(X_train, y_train)\ny_pred_cat = cat.predict(X_test)\nend = time.time()\nresults.append({\n    \"Model\": \"CatBoost\",\n    \"Accuracy\": round(accuracy_score(y_test, y_pred_cat), 4),\n    \"Training Time (s)\": round(end - start, 3),\n    \"Interpretability\": \"Medium - supports SHAP & feature effects\",\n    \"Overfitting Behavior\": \"Low - uses ordered boosting\"\n})\n\n# ----------------------------------------------------------\n# Display Results\n# ----------------------------------------------------------\ndf_results = pd.DataFrame(results)\nprint(\"=== Comparative Study of Ensemble Models ===\")\nprint(df_results.to_string(index=False))\n\n# ----------------------------------------------------------\n# Intermediate Question Answers\n# ----------------------------------------------------------\nprint(\"\\n=== Intermediate Question Answers ===\")\n\nprint(\"\\n1️⃣ Which model achieved the best performance?\")\nbest_model = df_results.loc[df_results['Accuracy'].idxmax()]\nprint(f\"→ {best_model['Model']} achieved the best accuracy of {best_model['Accuracy']}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:15:41.058144Z","iopub.execute_input":"2025-10-12T15:15:41.058465Z","iopub.status.idle":"2025-10-12T15:16:46.911227Z","shell.execute_reply.started":"2025-10-12T15:15:41.058442Z","shell.execute_reply":"2025-10-12T15:16:46.909819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It performed better because it uses advanced optimization and regularization techniques, \"\n      \"allowing it to capture complex patterns while controlling overfitting.","metadata":{}},{"cell_type":"markdown","source":"Bagging and Random Forest are simpler and relatively fast, but less flexible. \"\n      \"Boosting models (XGBoost, CatBoost) are more complex and take longer to train, \"\n      \"yet they offer higher accuracy and robustness. Interpretability generally decreases as model complexity increases.","metadata":{}},{"cell_type":"markdown","source":"XGBoost or CatBoost would be recommended for large datasets due to their scalability, \"\n      \"built-in regularization, and efficient parallel processing that handle real-world complexity effectively.","metadata":{}},{"cell_type":"code","source":"#8\n# ✅ FIXED CODE\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\n\n# 1️⃣ Generate a synthetic dataset\nX, y = make_classification(\n    n_samples=768, n_features=2, n_redundant=0, n_clusters_per_class=1,\n    random_state=42\n)\n\n# 2️⃣ Introduce missing data (10% missing values)\nrng = np.random.RandomState(42)\nmissing_mask = rng.rand(*X.shape) < 0.1\nX_missing = X.copy()\nX_missing[missing_mask] = np.nan\n\n# 3️⃣ Handle missing values using mean imputation\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X_missing)\n\n# 4️⃣ Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X_imputed, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 5️⃣ Train ensemble models\nmodels = {\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n}\n\n# 6️⃣ Visualize decision boundaries\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nx_min, x_max = X_imputed[:, 0].min() - 1, X_imputed[:, 0].max() + 1\ny_min, y_max = X_imputed[:, 1].min() - 1, X_imputed[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                     np.linspace(y_min, y_max, 200))\n\nfor ax, (name, model) in zip(axes, models.items()):\n    model.fit(X_train, y_train)\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.3)\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k', s=20)\n    ax.set_title(name)\n\nplt.suptitle(\"Decision Boundaries of Ensemble Models (with Missing Data)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T17:12:16.242117Z","iopub.execute_input":"2025-10-12T17:12:16.242358Z","iopub.status.idle":"2025-10-12T17:12:20.765461Z","shell.execute_reply.started":"2025-10-12T17:12:16.242337Z","shell.execute_reply":"2025-10-12T17:12:20.764309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#9\n# Step 1: Import libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Step 2: Define base models for heterogeneous ensemble\ndt = DecisionTreeClassifier(random_state=42)\nknn = KNeighborsClassifier()\nlr = LogisticRegression(max_iter=1000, random_state=42)\nsvm = SVC(probability=True, random_state=42)  # probability=True needed for soft voting\n\n# Step 3: Heterogeneous Ensemble (Voting Classifier)\nhetero_voting_hard = VotingClassifier(\n    estimators=[('dt', dt), ('knn', knn), ('lr', lr), ('svm', svm)],\n    voting='hard'\n)\nhetero_voting_soft = VotingClassifier(\n    estimators=[('dt', dt), ('knn', knn), ('lr', lr), ('svm', svm)],\n    voting='soft'\n)\n\n# Train heterogeneous ensembles\nhetero_voting_hard.fit(X_train, y_train)\nhetero_voting_soft.fit(X_train, y_train)\n\n# Predictions\ny_pred_hard = hetero_voting_hard.predict(X_test)\ny_pred_soft = hetero_voting_soft.predict(X_test)\n\n# Accuracy\nacc_hard = accuracy_score(y_test, y_pred_hard)\nacc_soft = accuracy_score(y_test, y_pred_soft)\n\nprint(f\"Heterogeneous Ensemble (Hard Voting) Accuracy: {acc_hard:.4f}\")\nprint(f\"Heterogeneous Ensemble (Soft Voting) Accuracy: {acc_soft:.4f}\")\n\n# Step 4: Homogeneous Ensemble Comparison\n# Using Bagging with Decision Tree\nbagging_dt = BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=50, random_state=42)\nbagging_dt.fit(X_train, y_train)\ny_pred_bag = bagging_dt.predict(X_test)\nacc_bag = accuracy_score(y_test, y_pred_bag)\nprint(f\"Homogeneous Ensemble (Bagging DT) Accuracy: {acc_bag:.4f}\")\n\n# Using Random Forest (another homogeneous ensemble)\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nacc_rf = accuracy_score(y_test, y_pred_rf)\nprint(f\"Homogeneous Ensemble (Random Forest) Accuracy: {acc_rf:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T17:35:04.16874Z","iopub.execute_input":"2025-10-12T17:35:04.169097Z","iopub.status.idle":"2025-10-12T17:35:04.808139Z","shell.execute_reply.started":"2025-10-12T17:35:04.169074Z","shell.execute_reply":"2025-10-12T17:35:04.807369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What makes an ensemble heterogeneous?\n\nA heterogeneous ensemble combines different types of models (e.g., Decision Tree, Logistic Regression, KNN, SVM) rather than multiple instances of the same model. Diversity in model types is key.","metadata":{}},{"cell_type":"markdown","source":"How does model diversity influence the final performance?\n\nDiverse models make different errors on the data. When combined, these errors can cancel out, leading to higher overall accuracy compared to a single model or homogeneous ensemble.","metadata":{}},{"cell_type":"markdown","source":"Which combination of models produced the most accurate ensemble and why?\n\nUsually, a combination that includes tree-based, linear, and distance-based models performs best because it covers complementary strengths:\n\nDecision Trees → capture non-linear patterns\n\nLogistic Regression → handles linear separability well\n\nKNN → captures local structure\n\nSVM → good for complex boundaries\n\nSoft voting tends to leverage probabilistic outputs better, often outperforming hard voting.","metadata":{}},{"cell_type":"markdown","source":"How does the voting method (hard vs. soft) impact the heterogeneous ensemble outcome?\n\nHard voting: final class is decided by majority vote.\n\nSoft voting: final class is decided by averaging predicted probabilities, which usually improves performance if models provide reliable probabilities.","metadata":{}},{"cell_type":"code","source":"#10\n# Step 1: Import Libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Step 2: Define base models\ndt = DecisionTreeClassifier(random_state=42)\nknn = KNeighborsClassifier()\nlr = LogisticRegression(max_iter=1000, random_state=42)\nsvm = SVC(probability=True, random_state=42)\n\n# -------------------------------\n# Homogeneous Ensembles\n# -------------------------------\n\n# 1. Bagging (Decision Trees)\nbagging_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n                               n_estimators=50, random_state=42)\nbagging_dt.fit(X_train, y_train)\ny_pred_bag = bagging_dt.predict(X_test)\nacc_bag = accuracy_score(y_test, y_pred_bag)\n\n# 2. Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nacc_rf = accuracy_score(y_test, y_pred_rf)\n\n# -------------------------------\n# Heterogeneous Ensembles\n# -------------------------------\n\n# 3. Voting Classifier\nvoting_soft = VotingClassifier(\n    estimators=[('dt', dt), ('knn', knn), ('lr', lr), ('svm', svm)],\n    voting='soft'\n)\nvoting_soft.fit(X_train, y_train)\ny_pred_voting = voting_soft.predict(X_test)\nacc_voting = accuracy_score(y_test, y_pred_voting)\n\n# 4. Stacking Classifier\nstacking = StackingClassifier(\n    estimators=[('dt', dt), ('knn', knn), ('lr', lr), ('svm', svm)],\n    final_estimator=LogisticRegression(),\n    cv=5\n)\nstacking.fit(X_train, y_train)\ny_pred_stacking = stacking.predict(X_test)\nacc_stacking = accuracy_score(y_test, y_pred_stacking)\n\n# -------------------------------\n# Print Results\n# -------------------------------\nprint(\"Ensemble Performance on Diabetes Dataset:\")\nprint(f\"Bagging (Homogeneous, DT) Accuracy: {acc_bag:.4f}\")\nprint(f\"Random Forest (Homogeneous) Accuracy: {acc_rf:.4f}\")\nprint(f\"Voting (Heterogeneous, Soft) Accuracy: {acc_voting:.4f}\")\nprint(f\"Stacking (Heterogeneous) Accuracy: {acc_stacking:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T17:37:56.145218Z","iopub.execute_input":"2025-10-12T17:37:56.145569Z","iopub.status.idle":"2025-10-12T17:37:56.960809Z","shell.execute_reply.started":"2025-10-12T17:37:56.145544Z","shell.execute_reply":"2025-10-12T17:37:56.959315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"| Ensemble Type     | Accuracy    | Computational Complexity | Interpretability                            | Robustness                                         |\n| ----------------- | ----------- | ------------------------ | ------------------------------------------- | -------------------------------------------------- |\n| **Bagging (DT)**  | Medium-High | Medium                   | Moderate (single tree easy, bagging harder) | High (reduces variance)                            |\n| **Random Forest** | High        | Medium-High              | Moderate (feature importance helps)         | Very High (resistant to overfitting)               |\n| **Voting (Soft)** | High        | Medium                   | High (easy to explain majority vote)        | High (combines different models)                   |\n| **Stacking**      | Very High   | High                     | Low (harder to explain meta-model)          | Very High (leverages strengths of multiple models) |\n","metadata":{}},{"cell_type":"markdown","source":"Which type of ensemble performed best overall and why?\n\nTypically, stacking achieves the highest accuracy because it leverages multiple model types and learns optimal weights via the meta-model. Random Forest also performs very well among homogeneous ensembles due to averaging many decorrelated trees.","metadata":{}},{"cell_type":"markdown","source":"What advantages did homogeneous ensembles show compared to heterogeneous ones?\n\nSimpler to train and tune.\n\nOften faster for large datasets if base models are identical.\n\nEasier to parallelize.\n\nMore interpretable in tree-based homogeneous ensembles (like Bagging DT or Random Forest).","metadata":{}},{"cell_type":"markdown","source":"How do computational cost and training time differ between the two approaches?\n\nHomogeneous ensembles: Training time is usually lower if base models are simple and identical; easier to parallelize.\n\nHeterogeneous ensembles: Higher cost because each model type may require different preprocessing, hyperparameters, and may not parallelize efficiently. Stacking is especially slow due to cross-validation for the meta-model.","metadata":{}},{"cell_type":"markdown","source":"What factors influence your choice between homogeneous and heterogeneous ensembles in real-world tasks?\n\nDataset size: Large datasets → homogeneous (easier to scale).\n\nNeed for interpretability: Homogeneous tree ensembles are easier to interpret.\n\nPerformance priority: Heterogeneous ensembles (voting, stacking) often achieve higher accuracy.\n\nComputational resources: Limited resources favor homogeneous ensembles.\n\nProblem complexity: Complex, non-linear, and heterogeneous data → heterogeneous ensembles perform better.","metadata":{}}]}